{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af23f723-2476-48da-8e1b-b5db90e9c3e6",
   "metadata": {},
   "source": [
    "# Plots features vs errors\n",
    "THE IDEA IS SEE IF THE VALUES OF SOME FEATURES HAS HIGHER ERRORS\n",
    "\n",
    "THIS HAS THE SAME IDEA OF PARTIAL DEPENDECE PLOT BUT INSTEAD OF PLOT y_predicted vs feature IN THIS NOTEBOOKS ARE PLOTEED errors vs feature\n",
    "\n",
    "\n",
    "---------\n",
    "Plots of predictions of the models with any kind of transformation\n",
    "\n",
    "**IMPORTANT**: The list of models to evaluate is the same, but each model could have its own feature eng, but the Input (the data_X) and the Output (the prediction) follow the same structure, so it is necesary only one notebook to evaluate the differents notebooks of training (if it is not logic for you thinking in the kaggle competitions).\n",
    "\n",
    "In this notebook, there are a parameter \"folder_models\" and in this folder are located the pkl of each model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15086bcf-d1a7-4eda-9774-34444f24f03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# fix root path to save outputs\n",
    "actual_path = os.path.abspath(os.getcwd())\n",
    "list_root_path = actual_path.split('\\\\')[:-1]\n",
    "root_path = '\\\\'.join(list_root_path)\n",
    "os.chdir(root_path)\n",
    "print('root path: ', root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d31f4e-c776-4d9d-8efe-055ef94e1edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "375f572f-cfd4-4e8e-a895-580672796d41",
   "metadata": {},
   "source": [
    "## RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c9c46b-0d53-4030-a0d8-55beed0ad1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e047b73-7c55-477c-a028-bfea0c567c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d80e6679-8838-49be-aa44-f6393a2189a0",
   "metadata": {},
   "source": [
    "### 0. Global params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c807f7f-fe2e-40a3-9182-732d9ad5a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define folder where the models were saved. There are the same models accepted by gurobi but the feature eng changed\n",
    "\n",
    "# list of folder with models = ['basic', 'scaler', 'poly_2', 'poly_3']\n",
    "folder_models = 'poly_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac559f84-4c10-4ebb-be4a-ba9440054d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f383b70-f1d9-4606-86d1-91f166caa58b",
   "metadata": {},
   "source": [
    "### 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7fed6c-92db-4682-822e-8d95392568ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE LIST FEARTURES - TARGET (order data to have the same order in the features always)\n",
    "list_features = ['AveOccup', 'Latitude', 'Population', 'AveBedrms', 'HouseAge', 'Longitude', 'AveRooms', 'MedInc']\n",
    "target = 'Price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e7496-7868-4436-b443-cdc415cbe977",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD DATA\n",
    "X_train = pd.read_pickle('artifacts/data/X_train.pkl')\n",
    "X_test = pd.read_pickle('artifacts/data/X_test.pkl')\n",
    "y_train = pd.read_pickle('artifacts/data/y_train.pkl')\n",
    "y_test = pd.read_pickle('artifacts/data/y_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab84813-d0cf-4984-b985-0ee7d96ff227",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape data')\n",
    "print('\\n\\n TRAIN')\n",
    "print('X_train: ', X_train.shape)\n",
    "print('y_train: ', y_train.shape)\n",
    "\n",
    "print('\\n\\n TEST')\n",
    "print('X_test: ', X_test.shape)\n",
    "print('y_test: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17ea9b7-8585-4830-a87d-c7fcdce765ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample - run fast\n",
    "X_train = X_train[0:100]\n",
    "y_train = y_train[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb083c-d2e2-4e10-97c3-84a8d4858836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a62564-70a5-4f91-9e38-1541fc78e9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51ef807b-a0a1-46c7-a9ad-65b023652cd3",
   "metadata": {},
   "source": [
    "### 2. Load Models\n",
    "Load all the models in a dictory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07259b0d-3fb9-4421-b230-bb4fecc999af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define list of models - list to have always the same order.\n",
    "#### In this example, the strings in the list are the same with the models were saved\n",
    "list_models_names = [\n",
    "    \"lr\",\n",
    "    \"ridge\",\n",
    "    \"lasso\",\n",
    "    \n",
    "    \"tree_simple\",\n",
    "    \"tree_default\",\n",
    "    \n",
    "    \"rf_simple\",\n",
    "    \"rf_default\",\n",
    "\n",
    "    \"gb_simple\",\n",
    "    \"gb_default\",\n",
    "\n",
    "    \"xgb_simple\",\n",
    "    \"xgb_default\",\n",
    "\n",
    "    \"mlp_simple\",\n",
    "    \"mlp_default\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e52f3b5-49b7-4b23-879c-b7b40fd96d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to folder models\n",
    "path_folder_models = f'artifacts/models/{folder_models}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53325c9-dfa6-4bbf-986c-db8cbd6fefcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load models\n",
    "dict_models = {}\n",
    "for model_name in list_models_names:\n",
    "    print(f'loading model: {model_name}')\n",
    "    path_model = path_folder_models + f'{model_name}.pkl'\n",
    "    with open(path_model, 'rb') as artifact:\n",
    "        dict_models[model_name] = pickle.load(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e26fb34-9575-46c2-ad9c-253297b444a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e256c715-2c79-43ce-b2c6-fda609319bdb",
   "metadata": {},
   "source": [
    "### 3. Define function to plot feature vs error (subplots each feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50827149-9b0f-4b50-a702-4ebf3e4ba373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_errors_vs_one_feature(X, y, y_pred, title_plot, list_features, abs_error):\n",
    "    \"\"\"\n",
    "    Plot errors vs features.\n",
    "\n",
    "    Args:\n",
    "        X (dataframe): dataframe with X true values - features\n",
    "        y (dataframe): dataframe with y-true values \n",
    "        y_pred (dataframe): dataframe with y-pred values\n",
    "        title_plot (string): tittle in the plot\n",
    "        list_features (list): list of features that will plot againts the errrors. The features needs to be present in the data\n",
    "        abs_error (boolean): True: Plot the absolute value of the errors abs(y_true - y_pred). False: Plot y_true - y_pred\n",
    "    \n",
    "    Return\n",
    "        fig (figure matplolib): figure to show, download, etc\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate error\n",
    "    errors = y - y_pred # error\n",
    "    errors_abs = np.abs(errors) # errors in abs value\n",
    "    if abs_error:\n",
    "        errors_to_plot = errors_abs\n",
    "    else:\n",
    "        errors_to_plot = errors\n",
    "\n",
    "    # for in features\n",
    "    for feature in list_features:\n",
    "\n",
    "        fig = plt.Figure()\n",
    "        plt.scatter(X[feature],  # feature\n",
    "                   errors_to_plot,  # errors y_true - y_pred\n",
    "                    alpha = 0.5\n",
    "                   )\n",
    "        \n",
    "        # Add names to axis\n",
    "        plt.xlabel(f'Feature: {feature}')\n",
    "        plt.ylabel('Errors y')\n",
    "\n",
    "        plt.title(title_plot)\n",
    "        #plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e629f3-f35b-4fd7-86a3-c6f9a2184b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## example\n",
    "\n",
    "# params\n",
    "model_example = dict_models['lr']\n",
    "y_example_true = y_train\n",
    "y_example_pred = model_example.predict(X_train)\n",
    "y_example_pred = pd.DataFrame(y_example_pred, index = y_example_true.index, columns = y_example_true.columns)\n",
    "\n",
    "# plot\n",
    "plot_errors_vs_one_feature(X = X_train,\n",
    "                               y = y_example_true, \n",
    "                        y_pred = y_example_pred, \n",
    "                        title_plot = 'lr train',\n",
    "                        list_features = list_features,\n",
    "                        abs_error = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a458435-07f2-4df4-82f3-adc167bf65d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e570627-8507-428d-be9b-82e07723d721",
   "metadata": {},
   "source": [
    "### 4. Define function to plot features (subplots) vs errors (One subplot for model)\n",
    "Have a for to plot all the models and save each model individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c7bdd-00e3-4eb5-a2a7-f4ebd8b026f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_errors_vs_all_features(model, X, y, abs_error):\n",
    "    \"\"\"\n",
    "    Plot errors vs features - all features in the data X\n",
    "\n",
    "    Args:\n",
    "        model (sklearn model): model sklearn\n",
    "        X (dataframe): dataframe with features\n",
    "        y (dataframe): dataframe with target (y_true)\n",
    "        abs_error (boolean): True: Plot the absolute value of the errors abs(y_true - y_pred). False: Plot y_true - y_pred\n",
    "    \n",
    "    Return\n",
    "        fig (figure matplolib): figure to show, download, etc\n",
    "    \"\"\"\n",
    "    \n",
    "    # predict y_pred\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred = pd.DataFrame(y_pred, index = y.index, columns = y.columns)\n",
    "    \n",
    "    # calculate error\n",
    "    errors = y - y_pred # error\n",
    "    errors_abs = np.abs(errors) # errors in abs value\n",
    "    if abs_error:\n",
    "        errors_to_plot = errors_abs\n",
    "    else:\n",
    "        errors_to_plot = errors\n",
    "\n",
    "    # create subplots\n",
    "    number_features = len(X.columns.tolist())\n",
    "    fig, ax = plt.subplots(number_features, 1, figsize = ((7, 50)) , dpi = 300)\n",
    "    \n",
    "    for index, feature in enumerate(X.columns.tolist()):\n",
    "    \n",
    "        # plot scatter feature vs error\n",
    "        ax[index].scatter(X[feature],  # feature\n",
    "                           errors_to_plot,  # errors\n",
    "                            alpha = 0.5\n",
    "                           )\n",
    "    \n",
    "        # Add names to axis\n",
    "        ax[index].set_xlabel(f'Feature: {feature}')\n",
    "        ax[index].set_ylabel('Errors y')\n",
    "    \n",
    "        # layout\n",
    "        ax[index].set_title(f'Plot Errors y - Feature: {feature}')\n",
    "    \n",
    "    \n",
    "    # Adjust vertical spacing between subplots\n",
    "    fig.subplots_adjust(wspace=0.5)\n",
    "    \n",
    "    # Automatically adjust layout to avoid overlapping elements\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22af913d-9c9c-435b-9dda-ea44ba06269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT FEATURES VS ERRORS - TRAIN DATA\n",
    "for index, model_name in enumerate(dict_models):\n",
    "    print(index)\n",
    "    print(model_name)\n",
    "\n",
    "    fig_true_pred_models_train = plot_errors_vs_all_features(model = dict_models[model_name],\n",
    "                                                             X = X_train,\n",
    "                                                             y = y_train,\n",
    "                                                             abs_error = True\n",
    "                                                            )\n",
    "    \n",
    "    # save plot\n",
    "    fig_true_pred_models_train.savefig(f'artifacts/plots_predictions_true_pred/{folder_models}/errors_features/plot_errors_features_{model_name}_train.png', \n",
    "                                       dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef4549f-68cf-47e3-a4a5-371afdf17086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT FEATURES VS ERRORS - TEST DATA\n",
    "for index, model_name in enumerate(dict_models):\n",
    "    print(index)\n",
    "    print(model_name)\n",
    "\n",
    "    fig_true_pred_models_test = plot_errors_vs_all_features(model = dict_models[model_name],\n",
    "                                                             X = X_test,\n",
    "                                                             y = y_test,\n",
    "                                                             abs_error = True\n",
    "                                                            )\n",
    "    \n",
    "    # save plot\n",
    "    fig_true_pred_models_test.savefig(f'artifacts/plots_predictions_true_pred/{folder_models}/errors_features/plot_errors_features_{model_name}_test.png', \n",
    "                                       dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e9d9e-7160-4773-ad46-48f09676979e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510cf7a2-7e92-40a2-8976-66ec99bdcee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b4dc68e-20c6-478b-947e-69574f2dbea4",
   "metadata": {},
   "source": [
    "### 5. Define function to plot features (subplots) vs errors (One subplot for model) (all models in one plot)\n",
    "Have a for to plot all the models and save each model individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a736351-ff9a-47a6-a2fb-947316534241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_errors_vs_feature_all_models(dict_models, X, y, feature, abs_error):\n",
    "    \"\"\"\n",
    "    Plot errors vs value of one features - plot for all models into one plot - plotly\n",
    "\n",
    "    Args:\n",
    "        dict_models(dictionary): python dictionary where each element are differents models\n",
    "        X (dataframe): dataframe with features\n",
    "        y (dataframe): dataframe with target (y_true)\n",
    "        feature (string): feature to plot\n",
    "        abs_error (boolean): True: Plot the absolute value of the errors abs(y_true - y_pred). False: Plot y_true - y_pred\n",
    "    \n",
    "    Return\n",
    "        fig (figure plotly): fig of plotly with the plot generated \n",
    "    \"\"\"\n",
    "    \n",
    "    # generate dataframe with y_true, y_pred, model\n",
    "    df_error = pd.DataFrame()\n",
    "    \n",
    "    for model_name, model in dict_models.items():\n",
    "        # calculate y_pred\n",
    "        y_pred = model.predict(X)\n",
    "        y_pred = pd.DataFrame(y_pred, index = y.index, columns = y.columns)\n",
    "    \n",
    "        # calculate error\n",
    "        errors = y - y_pred # error\n",
    "        errors_abs = np.abs(errors) # errors in abs value\n",
    "        if abs_error:\n",
    "            errors_to_plot = errors_abs\n",
    "        else:\n",
    "            errors_to_plot = errors\n",
    "    \n",
    "        # save dataframe\n",
    "        df_error_aux = pd.DataFrame({'error': errors_to_plot.values.flatten(), 'Model': model_name, f'{feature}': X[feature]})\n",
    "        df_error = pd.concat([df_error, df_error_aux], ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # plot scatter plot - y_true vs y_pred\n",
    "    fig = px.scatter(df_error, x = f'{feature}', y = 'error', color='Model', title='Errors vs Features by Model', opacity = 0.6)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36128c7f-9eae-4d8a-8b8c-16b6e67856c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT DATA TRAIN\n",
    "fig_true_pred_all_models_train = plot_errors_vs_feature_all_models(dict_models = dict_models, \n",
    "                                                                   X = X_train, \n",
    "                                                                   y = y_train,\n",
    "                                                                   feature = 'AveOccup',\n",
    "                                                                   abs_error = True\n",
    "                                                                  )\n",
    "\n",
    "# show\n",
    "#fig_true_pred_all_models_train\n",
    "\n",
    "# save\n",
    "fig_true_pred_all_models_train.write_html(f'artifacts/plots_predictions_true_pred/{folder_models}/errors_features/plot_errors_features_all_models_train.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e8035d-2453-4deb-9be5-df3a598a4bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42260ec1-0cf6-4028-ad51-a8a8ffc09f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c545e7d-9e16-4c1a-8b9a-9cd706262860",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- Plots with distribution of 2 features (axis x - axis y) and the error colored (like heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbccebc-4436-4eaf-a598-69e42bf8beac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
