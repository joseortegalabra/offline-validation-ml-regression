{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1553f09-840e-4f78-be5b-b86e24774f98",
   "metadata": {},
   "source": [
    "# Evaluate Metrics of Regressor Models\n",
    "Evaluate any kind of models models (with feature eng and without it)\n",
    "\n",
    "**IMPORTANT**: The list of models to evaluate is the same, but each model could have its own feature eng, but the Input (the data_X) and the Output (the prediction) follow the same structure, so it is necesary only one notebook to evaluate the differents notebooks of training (if it is not logic for you thinking in the kaggle competitions).\n",
    "\n",
    "In this notebook, there are a parameter \"folder_models\" and in this folder are located the pkl of each model\n",
    "\n",
    "The list of Metrics to evaluate are:\n",
    "\n",
    "\n",
    "**Group 1 R2**\n",
    "- R2\n",
    "\n",
    "**Group 2 MSE**\n",
    "- MSE\n",
    "\n",
    "**Group 3 RMSE**\n",
    "- RMSE\n",
    "- RMSE MEAN RATIO\n",
    "- RMSE IQR RATIO\n",
    "\n",
    "**Group 4 MAE**\n",
    "- MAE\n",
    "- MAE MEAN RATIO\n",
    "- MAE IQR RATIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13221f-2fc3-46b1-8ca0-951766f92e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# fix root path to save outputs\n",
    "actual_path = os.path.abspath(os.getcwd())\n",
    "list_root_path = actual_path.split('\\\\')[:-1]\n",
    "root_path = '\\\\'.join(list_root_path)\n",
    "os.chdir(root_path)\n",
    "print('root path: ', root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbcc241-d92f-44a2-b96d-8ce9fc6b21de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11359038-e77d-4e60-a4c7-5ee10848acb2",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81db1f99-91ea-42ec-9977-dd25bbe874b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd80d4-872e-44e6-a874-e5a9cfb54c80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fef10151-5baf-45ba-86f0-270a61e5377b",
   "metadata": {},
   "source": [
    "### 0. Global params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606aeff-ea8a-489c-9ed6-3c9fe6c4ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define folder where the models were saved. There are the same models accepted by gurobi but the feature eng changed\n",
    "\n",
    "# list of folder with models = ['basic', 'scaler', 'poly_2', 'poly_3']\n",
    "folder_models = 'poly_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090d919-3987-4927-bdcb-4f3dd81c8c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7a0fb9-2ddc-49ff-8db4-c364fa56aa07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ee5ebb7-3bb4-490e-9971-c58c5baca706",
   "metadata": {},
   "source": [
    "### 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6986d7-77b6-4b7f-b7db-d9edfc62f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE LIST FEARTURES - TARGET (order data to have the same order in the features always)\n",
    "list_features = ['AveOccup', 'Latitude', 'Population', 'AveBedrms', 'HouseAge', 'Longitude', 'AveRooms', 'MedInc']\n",
    "target = 'Price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b20aae-de76-4d14-98ba-5c6aae1d7406",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD DATA\n",
    "X_train = pd.read_pickle('artifacts/data/X_train.pkl')\n",
    "X_test = pd.read_pickle('artifacts/data/X_test.pkl')\n",
    "y_train = pd.read_pickle('artifacts/data/y_train.pkl')\n",
    "y_test = pd.read_pickle('artifacts/data/y_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5fb8bc-3d00-400b-8dd7-a38dbed0184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape data')\n",
    "print('\\n\\n TRAIN')\n",
    "print('X_train: ', X_train.shape)\n",
    "print('y_train: ', y_train.shape)\n",
    "\n",
    "print('\\n\\n TEST')\n",
    "print('X_test: ', X_test.shape)\n",
    "print('y_test: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d92f3-9b89-4c5d-ba8a-8d842b596c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "214f0db3-646e-4258-9a14-29238415c10f",
   "metadata": {},
   "source": [
    "### 2. Load Models\n",
    "Load all the models in a dictory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345d470a-4a81-42fa-888d-165ac5978cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define list of models - list to have always the same order.\n",
    "#### In this example, the strings in the list are the same with the models were saved\n",
    "list_models_names = [\n",
    "    \"lr\",\n",
    "    \"ridge\",\n",
    "    \"lasso\",\n",
    "    \n",
    "    \"tree_simple\",\n",
    "    \"tree_default\",\n",
    "    \n",
    "    \"rf_simple\",\n",
    "    \"rf_default\",\n",
    "\n",
    "    \"gb_simple\",\n",
    "    \"gb_default\",\n",
    "\n",
    "    \"xgb_simple\",\n",
    "    \"xgb_default\",\n",
    "\n",
    "    \"mlp_simple\",\n",
    "    \"mlp_default\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acdd849-423f-4697-b3fe-a885d1b0d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to folder models\n",
    "path_folder_models = f'artifacts/models/{folder_models}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d019a-6005-4d78-b105-eebb4ab1a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load models\n",
    "dict_models = {}\n",
    "for model_name in list_models_names:\n",
    "    print(f'loading model: {model_name}')\n",
    "    path_model = path_folder_models + f'{model_name}.pkl'\n",
    "    with open(path_model, 'rb') as artifact:\n",
    "        dict_models[model_name] = pickle.load(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd849e-07e5-4101-b0c4-0b64ff86ca23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf94a7-d9cc-436c-a41d-7328bd9571d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6957096-823d-474e-9823-147fe156df95",
   "metadata": {},
   "source": [
    "### 3. Define Functions to calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9673d1fe-e4ba-498d-9334-bd1c498d4b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show version scikit-learn - since version 1.4 some codes to evaluate metrics changed\n",
    "!pip show scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b3ed35-3ae4-491d-89c4-f70ba12fb677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_regressors_models(y, y_pred, model_name, decimals_round = None):\n",
    "    \"\"\"\n",
    "    Calculate a certain number of metrics to evaluate regression models. The metrics are rounded to X decimals\n",
    "\n",
    "    Args\n",
    "        y (dataframe): y true\n",
    "        y_pred (dataframe): y predicted with the model. In this codes are passed y_pred instead of X\n",
    "        model_name (string): name of the model. This name is used when the metrics are saved to identify the model of these metrics\n",
    "        decimals_round = Number of decimals to round the values. Defult None, no round the values.\n",
    "\n",
    "    Return\n",
    "        metrics_regressors (dataframe): dataframe with the metrics of the model in this datasets. Row: name metrics. Columns: value metrics\n",
    "    \"\"\"\n",
    "\n",
    "    #### R2\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    #### MSE\n",
    "    mse = mean_squared_error(y, y_pred, squared = True)\n",
    "    \n",
    "    #### RMSE\n",
    "    rmse = mean_squared_error(y, y_pred, squared = False)\n",
    "    \n",
    "    #### RMSE_MEAN_RATIO\n",
    "    # rmse mean ratio: rmse / mean_y_true\n",
    "    rmse_mean_ratio = rmse / y.mean().values[0]\n",
    "    rmse_mean_ratio = round(100 * (rmse_mean_ratio), 2)\n",
    "    \n",
    "    #### RMSE_IQR_RATIO\n",
    "    # rmse iqr ratio: rmse / iqr_y_true\n",
    "    rmse_iqr_ratio = rmse / iqr(y)\n",
    "    rmse_iqr_ratio = round(100 * (rmse_iqr_ratio), 2)\n",
    "    \n",
    "    #### MAE\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    \n",
    "    #### MAE_RATIO\n",
    "    mae_mean_ratio = mae / y.mean().values[0]\n",
    "    mae_mean_ratio = round(100 * (mae_mean_ratio), 2)\n",
    "    \n",
    "    #### MAE_IQR_RATIO\n",
    "    mae_iqr_ratio = mae / iqr(y)\n",
    "    mae_iqr_ratio = round(100 * (mae_iqr_ratio), 2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #### JOIN INTO ONE DATAFRAME\n",
    "    # create dataframe\n",
    "    metrics_regressors = pd.DataFrame(index = [model_name])\n",
    "    \n",
    "    # add metrics\n",
    "    metrics_regressors['r2'] = r2\n",
    "    metrics_regressors['mse'] = mse\n",
    "    metrics_regressors['rmse'] = rmse\n",
    "    metrics_regressors['rmse_mean_ratio(%)'] = rmse_mean_ratio\n",
    "    metrics_regressors['rmse_iqr_ratio(%)'] = rmse_iqr_ratio\n",
    "    metrics_regressors['mae'] = mae\n",
    "    metrics_regressors['mae_mean_ratio(%)'] = mae_mean_ratio\n",
    "    metrics_regressors['mae_iqr_ratio(%)'] = mae_iqr_ratio\n",
    "    \n",
    "    # round\n",
    "    metrics_regressors = metrics_regressors.astype('float')\n",
    "    if decimals_round:\n",
    "        metrics_regressors = metrics_regressors.round(decimals_round)\n",
    "\n",
    "\n",
    "    return metrics_regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8786b0b5-2777-4b92-bb0f-79a33086aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show examples\n",
    "calculate_metrics_regressors_models(y = y_train,\n",
    "                                    y_pred = dict_models['lr'].predict(X_train),\n",
    "                                    model_name = 'lr',\n",
    "                                    decimals_round = 3\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a946575-f1bc-4f9c-b740-ae7f35127830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf01c078-4cf7-46ae-aac2-a526e5417891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4d2ddbd-4ff4-4196-8cc7-e770523360fe",
   "metadata": {},
   "source": [
    "### 4. Calculate metrics train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21acb45-59d3-4bf8-bf99-625a6465e986",
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculate metrics for all models, TRAIN DATA\n",
    "metrics_train = pd.DataFrame()\n",
    "for m_name in list_models_names:\n",
    "    print(f'calculating metrics: {m_name}')\n",
    "\n",
    "    # calcualte metrics\n",
    "    y_pred_train = dict_models[m_name].predict(X_train)\n",
    "    metrics_aux = calculate_metrics_regressors_models(y = y_train,\n",
    "                                                      y_pred = y_pred_train,\n",
    "                                                      model_name = m_name,\n",
    "                                                      decimals_round = 3\n",
    "                                                     )\n",
    "\n",
    "    # append ouput dataframe\n",
    "    metrics_train = pd.concat([metrics_train, metrics_aux], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d551caea-c68e-4939-b3c1-5b25dd9ce96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e817536-06fa-4370-80b0-c2fa52ebbf97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a122f2a0-7a6c-49eb-9e99-23a218a3eea1",
   "metadata": {},
   "source": [
    "### 5. Calculate metrics test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2e74db-7512-4198-a846-4f2890669705",
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculate metrics for all models, TEST DATA\n",
    "metrics_test = pd.DataFrame()\n",
    "for m_name in list_models_names:\n",
    "    print(f'calculating metrics: {m_name}')\n",
    "\n",
    "    # calcualte metrics\n",
    "    y_pred_test = dict_models[m_name].predict(X_test)\n",
    "    metrics_aux = calculate_metrics_regressors_models(y = y_test,\n",
    "                                                      y_pred = y_pred_test,\n",
    "                                                      model_name = m_name,\n",
    "                                                      decimals_round = 3\n",
    "                                                     )\n",
    "\n",
    "    # append ouput dataframe\n",
    "    metrics_test = pd.concat([metrics_test, metrics_aux], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b8fb58-4078-44ce-8f4c-0e09a4bf848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948ce379-1986-4905-97e5-94acaed201b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9425fd34-5dd4-4ddf-8e93-73238bf8980b",
   "metadata": {},
   "source": [
    "### 6. Save Metrics\n",
    "Save metrics in a excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f7255-4058-4523-bc63-a1303e2055f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_train.to_excel(f'artifacts/metrics/{folder_models}/metrics_train.xlsx')\n",
    "metrics_test.to_excel(f'artifacts/metrics/{folder_models}/metrics_test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4db9f03-d92c-4e53-a991-19db20bc3b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384fc771-3847-432a-850b-e160dfef9960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
